# Speeding up iteration procedures

At this stage, we assume that you've grown familiar with R's most relevant loop 
constructs, including `for` loops and the `*apply` family of functions. Yet, 
there's two other packages we'd like to introduce within the scope of this short 
course due to their convenient performance when it comes to accelerating certain 
operations. But first things first... here's the topics (and the related 
packages) that we're gonna cover in the upcoming section on speeding up 
iteration procedures in R.

<hr>

<b>Parallelization via `doParallel` (and `foreach`)</b>

Just a bit of theory ahead.

<blockquote>
"Speedup is a common measure of the performance gain from a parallel processor. 
It is defined as the ratio of the time required to complete the job with one 
processor to the time required to complete the job with <i>N</i> processors. 
[...] In a machine where work can be dynamically assigned to available 
processors, it is attained as long as the number of pieces of work ready for 
processing is at least <i>N</i>." [@Denning1990]
</blockquote>

Or, to cut a long story short, if you're performing one and the same iteration 
again and again, you may as well distribute an equal amount of sub-iterations to 
each processor available on your local machine. In theory, distributing an 
operation to <i>N</i> nodes would result in a 10-fold speed gain. 

<center>
  <img src="https://i2.wp.com/gforge.se/wp-content/uploads/2015/02/Horse_power_smudge_9000.jpg" alt="horses" style="width: 500px;"/>
</center>

Enough theory - let's proceed to actual practice. We'll have a brief look at R's 
capabilities in terms of parallel processing using the **doParallel** package 
along with **foreach** (an other package to deal with loop constructs). Be 
aware, however, that there are plenty of opportunities to ["go parallel" in R](http://www.r-bloggers.com/how-to-go-parallel-in-r-basics-tips/) which you 
might want to have a look at.

<hr>

<b>"Cross-lingual" programming via `Rcpp`</b>

The **Rcpp** package offers a seamless integration of C++ functionality in R. 
The underlying reason why such a thing exists in the first place is rather easy 
to explain: sometimes R code is just not fast enough. But don't be afraid if you 
haven't gotten in touch with C++ so far: we're barely gonna scratch the surface 
of what is possibly when combining those two languages. In fact, our short 
excursion on the topic is merely meant to raise awareness of such things 
slumbering in the depth of the R universe. In case you'd like to learn more 
about the subject, we recommend to have a closer look at Hadley Wickham's 
comprehensive [Rcpp tutorial on Advanced R](http://adv-r.had.co.nz/Rcpp.html) or, 
if you're a fan of hard copies, Dirk Eddelbuettel's book about "Seamless R and 
C++ Integration with Rcpp". 

<center>
  <img src="http://www.rcpp.org/book/seamless.png" alt="rcpp" style="width: 300px;"/>
</center>


## Parallelization

As regards multi-core operations, there's actually a whole bunch of packages 
that allow you to distribute particular processing steps to multiple cores on 
your local machine. Out of those, we decided to focus on **doParallel** (in 
conjunction with **foreach**) as it represents a straightforward solution that 
does not require the user to manually export objects from the global environment 
to the single nodes (e.g. required by `par*apply` along with **parallel**).

<hr>

<b>Getting started</b>

Let's start straight away with detecting the number of cores available on your 
machine via `detectCores`. In the end, there's no use in trying to split a 
certain task into 5 parts when there are only 4 horses available, is it? &#128521; 

```{r ncores, message=FALSE, warnings = FALSE}
## load 'doParallel' package
library(doParallel)

## no. of local cores
detectCores()
```

In our case, there are 4 nodes available for parallelization among which 
iteration procedures may be divided. Note that this number typically varies 
between computers. Note also that `library(doParallel)` automatically attaches 
the **foreach** package which we will require later on, so there's no need to 
manually load it. In fact, this would only be required if we decided to use 
`foreach` on a single core, but for such operations, we highly recommend to use 
`*apply`.

Next, let's create a socket cluster for parallel processing via `makeCluster`. 
Think of this as a set of workers among which a particular work step should be 
divided equally - just like in the image on the previous page. With 
`registerDoParallel`, we tell R that it should use the cluster 'cl' for any 
further multi-core operations performed with `foreach`.

```{r cluster, warnings = FALSE}
## create parallel socket cluster
cl <- makeCluster(3)

## register parallel backend
registerDoParallel(cl)
```

You will probably notice that we told R to use 3 nodes to work with. Why not use  
all 4 nodes?, you might wonder. Well, parallel processing distributed among all 
nodes available tends to slow down your computer considerably. Since we'd 
possibly like to perform some other actions while R is occupied (e.g. browsing 
the internet, checking e-mails, etc.), it is wise to leave some remaining 
computational power for such operations as well.  

<hr>

<b>The `foreach` syntax</b>

Now that we're set up properly, it's time to give our cluster something to work 
with. Remember the previous example on calculating the linear relationship 
between 'carat' and 'price' for each 'cut' quality iteratively? We're gonna do the same thing again now 
except this time it's `foreach` we'll be working with. Similar to `*apply`, 
`foreach` requires a set of input data and a function to perform on the very same. 
Have a look at the following example. 

```{r foreach, warning = FALSE}
## calculate square root, return list
foreach(i = 1:4) %do% sqrt(i)
```

Similar to `lapply`, `foreach` returns an object of class 'list' by default. The 
function body, by contrast, is connected with the part on variable 
definition via the binary operator `%do%`. This might seem odd at first, but you 
will notice later on that this syntax comes in quite handy when forwarding 
`foreach`-related operations to a parallel cluster. 

We may easily change the output type of `foreach` by telling R how to `.combine` 
the results of the single iterations via 

```{r combine, warning = FALSE}
## calculate sqare root, return vector
foreach(i = 1:4, .combine = "c") %do% sqrt(i)
```

et voil√†, we transformed the `lapply`-style list output of our loop into an 
`sapply`-style vector output.  

<hr>

<b>Task: apply `lm` to selected columns (using `foreach`)</b>

Let's now come back to the 'diamonds' dataset. Since you may just take the 
function body from the previous application with `lapply`, it's up to you to 
calculate the linear model between 'carat' and 'price' for each group of 'cut'. 
Remember to `split` the 'diamonds' dataset first and pass the thus created list 
(with each list entry representing a group of uniform cuts) to `foreach`.

<center>
  <img src="https://upload.wikimedia.org/wikipedia/commons/2/25/Hourglass_2.svg" alt="hourglass" style="width: 125px;"/>
</center>

```{r lm_foreach, eval = FALSE, echo = FALSE, warnings = FALSE}
## load 'ggplot2' and split 'diamonds' dataset
library(ggplot2)
ls_diamonds <- split(diamonds, f = diamonds$cut)

## for each group of cuts, calculate lm between carat and price
ls_lm <- foreach(i = ls_diamonds) %do% lm(carat ~ price, data = i)

## using `sapply`, extract the corresponding r-squared values
rsq <- sapply(ls_lm, function(i) summary(i)$r.squared)
names(rsq) <- unique(diamonds$cut)
rsq
```

<hr>

<b>How *not* to use parallel features</b>

Let's see how long this code chunk takes to perform. The **microbenchmark** 
package is just the right thing for such an operation.

```{r lm_foreach_mb, warning = FALSE}
## load 'microbenchmark' package
library(microbenchmark)

## load 'ggplot2' and split 'diamonds' dataset
library(ggplot2)
ls_diamonds <- split(diamonds, f = diamonds$cut)

## speed test (this might take some time)
microbenchmark({
  foreach(i = ls_diamonds) %do% lm(carat ~ price, data = i)
}, times = 20L) # number of times to evaluate the expression
```

Hm, quite some time. Let's see how long it takes on when using multiple cores. 
The only thing that's required in order to let this operation run on multiple 
cores is to replace `%do%` with `%dopar%`. In doing so (and distributing the 
iterative `lm` calculation on multiple cores), the operation should perform 
much faster, right?

```{r lm_foreach_par_mb, warning = FALSE}
microbenchmark({
  foreach(i = ls_diamonds) %dopar% lm(carat ~ price, data = i)
}, times = 20L)
```

Oops, what's going on now? Obviously, this action doesn't perform faster at all 
although we told R to run in parallel via `%dopar%`. In fact, this is a bad 
example for the parallelized use of `foreach`. `lm` is a highly optimized base-R 
function that performs quite fast without the need to go parallel. "With [such] 
small tasks, the overhead of scheduling the task and returning the result can be 
greater than the time to execute the task itself, resulting in poor 
performance." [@Weston2014]

<hr>

<b>How to use parallel features</b>

Now that you've learned how *not* to use `%dopar%`, let's see what a proper use 
would look like. 'Proper' in this prospect refers to a piece of code that is 
computationally expensive and needs to be repeated at least a couple of times. 
For example, let's assume we wanted to separately predict 'cut', 'color' and 
'carat' for each specimen from 'diamonds' based on all remaining variables. 
Using `foreach` (or `lapply`), the referring code would roughly look as follows.


```{r foreach_ctree, eval = TRUE, message = FALSE, warnings = FALSE}
## load 'party' package
library(party)

system.time({
  
  ## conditional inference trees 
  ls_ct <- foreach(i = c("cut", "color", "carat")) %do% {
  
    # formula
    frml <- as.formula(paste(i, ".", sep = " ~ "))
  
    # classification
    ctree(frml, data = diamonds,
          controls = ctree_control(testtype = "MonteCarlo",
                                   nresample = 999,
                                   mincriterion = 0.999,
                                   maxdepth = 3))
  }
})
```

`ctree` performs rather slowly, which is particularly owing to `nresample` that 
tells the function to perform 1000 internal Monte-Carlo replications. Luckily, 
we can easily split this operation into 3 parts, i.e. one node takes 'cut' as 
response variable, another node 'color' and a third node 'carat' - at the same 
time!

```{r foreach_ctree_par, eval = TRUE, warning = FALSE}
system.time({
  
  ## conditional inference trees 
  ls_ct <- foreach(i = c("cut", "color", "carat"), 
                   .packages = c("ggplot2", "party")) %dopar% {
  
    # formula
    frml <- as.formula(paste(i, ".", sep = " ~ "))
  
    # classification
    ctree(frml, data = diamonds,
          controls = ctree_control(testtype = "MonteCarlo",
                                   nresample = 999,
                                   mincriterion = 0.999,
                                   maxdepth = 3))
  }
})
```

Of course, this is seconds we are talking about. Nonetheless, everyone of you 
will eventually end up with quite big datasets upon which computationally 
expensive operations need to be performed in an iterative manner. It might be 
hours or even days (trust us, we know what we're talking about...) that one 
single operation takes to perform - and then, the `%dopar%` might come in quite 
handy.

<hr>

<b>Closing a parallel backend</b>

One final remark on the proper use of parallel backends in R. When working on 
multiple cores, you can easily lose track of how many parallel backends, if any, 
you registered during your current session, especially when some error prevents 
your script from finishing. If you should ever find yourself in such a 
situation, do not hesitate to use `showConnections` to print information on 
currently open connections to the R console. 

```{r show_conn}
showConnections()
```

There's 3 socket connections (i.e. cores) registered at the moment, just as we 
initially defined via `registerDoParallel`. In order to close these connections 
(which we recommend to consider at the end of each parallelized R script), 
simply perform 

```{r close_conn}
stopCluster(cl)
```

which explicitly deregisters the implicitly created cluster (except for the 
'textConnection' required to create this very document, of course &#128521;).

```{r show_conn2}
showConnections()
```


## C++ interconnectivity via **Rcpp**

<b>Prerequisites</b>

In order for **Rcpp** to work, we have to make sure your local system is capable 
of building packages. On Linux-based systems, this shouldn't be much of a 
problem since things just generally tend to work whereas on Windows (or OS X), 
you will possible be required to install 
[**Rtools**](https://cran.r-project.org/bin/windows/Rtools/) 
(or [**Xcode**](https://itunes.apple.com/us/app/xcode/id497799835?mt=12)) to be 
able to compile C++ functions and make them available in R. Further information 
on package development prerequisites can be found 
[here](https://support.rstudio.com/hc/en-us/articles/200486498-Package-Development-Prerequisites). 

You may easily check if everything works by running the following code chunk.

```{r cpp_setup}
## load 'Rcpp' package
library(Rcpp)

## try to evaluate c++ expression
evalCpp("1 + 1")
```

If this expression does <u>not</u> evaluate to '2', there's something wrong with 
your local setup and you should possibly contact one of the lecturers for 
troubleshooting.

<hr>

<b>How to make your C++ code available in R</b>

**Rcpp** offers two ways to import C++ functions into R, namely

* `cppFunction` and
* `sourceCpp`. 

While the former takes an entire C++ source code as input argument (`code`), the 
latter behaves very similar to base-R `source` in the sense that it sources a 
code file (.cpp) and makes the functions included therein available in your  
global R environment. During this short overview, however, we'll primarily focus 
on the first approach while the latter is introduced only briefly.

<hr>

<i>No input, scalar output</i>

No matter which approach you will use in the end, **Rcpp** will take the C++ 
code, compile it and transform it into a proper R function. Imagine, for 
instance, the following code (which is heavily based on Hadley Wickham's 
[Advanced R. High performance functions with Rcpp](http://adv-r.had.co.nz/Rcpp.html) 
tutorial). 

```{r cpp_ex1}
## function that returns '1'
cppFunction('int one() {
  return 1;
}')

one()
```

Note that C++ requires you to specify the output type of `one()` which, in this 
particular case, is obviously an integer (`int`). Accordingly, R variables of 
type 'numeric', 'character' and 'logical' are referred to as `double`, `String` 
and `bool` in C++ language. Let's move on to our next example.

<i>Scalar input, scalar output</i>

When working with C++ code, you are required to not only specify the output 
type of your function, but also the type(s) of the input argument(s). The next 
code chunk defines a function `signC` that takes an integer input `x` and, 
depending on the arithmetic sign of `x`, returns one of '1', '0' and '-1'. 

```{r cpp_ex2}
## function that returns 1 if 'x' is positive, -1 if 'x' is negative, 0 otherwise
cppFunction('int signC(int x) {
  if (x > 0) {
    return 1;
  } else if (x == 0) {
    return 0;
  } else {
    return -1;
  }
}')

signC(-10)
```

Note also that `if` statements in C++ look very similar to their R equivalent. 
The only obvious differences are 

* the need to explicitly include a `return` statement and 
* the semicolons (`;`) terminating each line of code.

<i>Vector input, scalar output</i>

Although this seems hardly necessary, let's assume for now that we wanted to 
rewrite the base-R `sum` function in C++. Rather than supplying a scalar input 
argument, we need the function to work with a vector of numbers for obvious 
reasons. Similar to the different scalar inputs depicted above, base-R 
'integer', 'numeric', 'character' and 'logical' vectors are represented as 
`IntegerVector`, `NumericVector`, `CharacterVector` and `LogicalVector` in C++. 
This time, it also makes sense to define the input type as `NumericVector` and, 
accordingly, the output type as `double` since we'd possibly like to supply 
numbers with decimal places instead of raw integers.

```{r cpp_ex3}
cppFunction('double sumC(NumericVector x) {
  int n = x.size();
  double total = 0;
  for(int i = 0; i < n; ++i) {
    total += x[i];
  }
  return total;
}')

sumC(seq(0, 1, 0.1))
```

<i>Matrix input, vector output</i>

**Rcpp** also comes with a number of so-called 'sugars' that help newcomers to 
find their way by providing C++-equivalents of a number of built-in R functions. 
A short overview of featured functions is e.g. given by 
@Eddelbuettel2011. Among others, these include 

* _math functions_, e.g. `abs`, `ceiling`, `floor`, `exp`, `log`
* _scalar summaries_, e.g. `mean`, `min`, `max`, `sum`, `sd`, `var`
* _vector summaries_, e.g. `cumsum`, `diff`
* _finding utilities_, e.g. `which_max`, `which_min`, `match`
* _finding duplicates_, e.g. `duplicated`, `unique`

In order to demonstrate the proper use of such 'sugars' and, at the same time, 
introduce `NumericMatrix` (`NumericVector`) as further input (output) variable 
types, let's replicate the previous example on the use of `apply` to calculate 
mean values from each single variable column of the 'diamonds' dataset using 
**Rcpp** functionality. For the sake of simplicity of this demonstration, let's 
again focus on the numeric columns only (note also the use of `sapply` to create 
an index vector of (non-)numeric columns). 

```{r colMeansC, message = FALSE}
## subset with numeric columns only
library(ggplot2)

num_cols <- sapply(1:ncol(diamonds), function(i) is.numeric(diamonds[, i]))
diamonds_sub <- as.matrix(diamonds[, num_cols])

## c++-version of 'colMeans'
cppFunction("NumericVector colMeansC(NumericMatrix x) {
  
  // number of rows and columns
  int nCol = x.ncol();
  int nRow = x.nrow();
  
  // temporary variable of size nrow(x) to store column values in
  NumericVector nVal(nRow);
  
  // initialize output vector
  NumericVector out(nCol);
  
  // loop over each column
  for (int i = 0; i < nCol; i++) {
    
    // values in current column
    nVal = x(_, i);
    
    // store mean of current 'nVal' in 'out[i]'
    out[i] = mean(nVal);
  }
  
  return out;
}")

means <- colMeansC(diamonds_sub)
names(means) <- colnames(diamonds_sub)
means

## speed check
library(microbenchmark)

microbenchmark(
  val_apply <- apply(diamonds_sub, 2, mean), 
  val_cpp <- colMeansC(diamonds_sub)
, times = 20L)

## similarity check
identical(val_apply, means)
```

It's milliseconds we are talking about here, but still - `colMeansC` runs 
more than 5 times faster as compared to the `apply` approach!

<hr>

<b>What's the point of that?</b>

You might guess that we did not decide to include this chapter on C++ 
interconnectivity just for fun. The actual reason is that C++ code performs much 
faster as compared to R when it comes to `for` loops. Without going too much 
into detail, one of the underlying reason is the very efficient memory 
management of the C++ language as compared to the massive overhead that R 
produces during each intermediary step. But find out for yourselves...

<hr>

<b>Task: `sumR` vs. `sumC`</b>

Write a function `sumR` (do <u>not</u> use the built-in `sum` function) as an 
equivalent to the above `sumC` function and have a look at the time it takes to 
run `sumR(1:1e6)` using `system.time` (or `microbenchmark`). 

<center>
  <img src="https://upload.wikimedia.org/wikipedia/commons/2/25/Hourglass_2.svg" alt="hourglass" style="width: 125px;"/>
</center>

```{r sumR, echo = FALSE}
sumR <- function(x) {
  
  ## initialize input and output vector
  out <- 0
  
  ## add up values of 'x'
  for (i in 1:length(x))
    out <- out + x[i]
  
  ## return 'out'
  return(out)
}
```

```{r speed_check, warning = FALSE}
## speed check
microbenchmark(
  sum(1:1e6), # built-in `sum` function
  sumR(1:1e6), # base-R version
  sumC(1:1e6)  # rcpp version
, times = 20L)
```

As you can see, `sumC` runs more than 40 times faster than `sumR` and, at the 
same time, performs only slightly slower as compared to the highly optimized 
(vectorized) built-in `sum` function. You cannot imagine what's possible with 
**Rcpp** when it comes to more complex operations!

<hr>

<b>A short note on the use of `sourceCpp`</b>

For such short operations, the use of `cppFunction` seems reasonable. However, 
we recommend to use `sourceCpp` when it comes to more complex C++ functions. 
Take, for example, the following peace of C++ code that reproduces the built-in 
`cor` function. 

```{r cor_cppFunction, eval = FALSE}
cppFunction('double corC(NumericVector x, NumericVector y) {
  int nx = x.size(), ny = y.size();
  
  if (nx != ny) stop("Input vectors must have equal length!");
  
  double sum_x = sum(x), sum_y = sum(y);
  
  NumericVector xy = x * y;
  NumericVector x_squ = x * x, y_squ = y * y;
  
  double sum_xy = sum(xy);
  double sum_x_squ = sum(x_squ), sum_y_squ = sum(y_squ);
  
  double out = ((nx * sum_xy) - (sum_x * sum_y)) / sqrt((nx * sum_x_squ - pow(sum_x, 2.0)) * (nx * sum_y_squ - pow(sum_y, 2.0)));
  
  return out;
}')
```

Quite confusing, isn't it? Not the least because the inline C++ is not formatted 
properly. Luckily, RStudio comes with a C++ editor that allows you to write 
stand-alone .cpp functions - including code formatting! For that purpose, 
select 'C++ file' from the top-left dropdown menu and paste the code that we 
initially passed as `code` argument to `cppFunction`. 

<center>
  <img src="http://i.imgur.com/nKf0QZH.png" alt="new_cpp" style="width: 125px;"/>
</center>

In order to ensure compatibility with **Rcpp** and make the C++ function 
available in R, we need to add a header to our .cpp file (see below). In the 
end, our .cpp file should look as follows.

```{r engine='Rcpp'}
#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double corC(NumericVector x, NumericVector y) {
  int nx = x.size(), ny = y.size();
  
  if (nx != ny) stop("Input vectors must have equal length!");
  
  double sum_x = sum(x), sum_y = sum(y);
  
  NumericVector xy = x * y;
  NumericVector x_squ = x * x, y_squ = y * y;
  
  double sum_xy = sum(xy);
  double sum_x_squ = sum(x_squ), sum_y_squ = sum(y_squ);
  
  double out = ((nx * sum_xy) - (sum_x * sum_y)) / sqrt((nx * sum_x_squ - pow(sum_x, 2.0)) * (nx * sum_y_squ - pow(sum_y, 2.0)));
  
  return out;
}
```

Save the file in 'src/corC.cpp' and return to R, then run

```{r cor_sourceCpp}
## source 'corC' function (remember to adjust the path)
sourceCpp("src/corC.cpp")

## correlation of 'carat' and 'price'  
microbenchmark(
  cor(diamonds$carat, diamonds$price), 
  corC(diamonds$carat, diamonds$price) 
, times = 20L)
```

Note that, again, `corC` performs only slightly slower than the built-in `cor` 
function. Just imagine the speed gain as compared to a possible `corR` function!