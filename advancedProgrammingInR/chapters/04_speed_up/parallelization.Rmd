```{r knitsetup, echo=FALSE, results='hide', warning=FALSE, message=FALSE, cache=FALSE}
library(knitr)
library(knitcitations)
mybib <- bibtex::read.bib("../../bib/references")
opts_knit$set(base.dir='./', fig.path='', out.format='md')
opts_chunk$set(prompt=TRUE, comment='', results='markup')
# See yihui.name/knitr/options for more Knitr options.
##### Put other setup R code here


# end setup chunk
```
# Parallelization

There's actually a whole bunch of packages that allow you to split particular 
tasks to multiple cores on your local machine. Out of those, we decided to focus 
on **doParallel** (in conjunction with **foreach**) as it represents a straightforward 
solution that does not require the user to manually export objects from the 
global environment to the single nodes (e.g. required by **parallel**).

Let's start with detecting the number of cores available on your machine by 
running the following piece of code. 

```{r ncores, message=FALSE}
## load 'doParallel' package
library(doParallel)

## no. of local cores
detectCores()
```

In my particular case, there are 4 nodes available for parallelization. However, 
this number typically varies between computers. Note also that `library(doParallel)` 
automatically attaches the **foreach** package which we will require later on. 
Next, let's create and register a socket cluster for parallel processing. Just think of 
this as a set of workers among which a particular work step should be divided 
equally. With `registerDoParallel`, we tell R that it should use the created 
cluster for any further multi-core operations performed with `foreach`.

```{r cluster}
## create parallel socket cluster
cl <- makeCluster(3)

## register parallel backend
registerDoParallel(cl)
```

You will probably notice that we told R to use 3 nodes to work with. Why's he 
doing that?, you might wonder since our machine is equipped with 4 nodes in 
total. Well, parallel processing distributed among all nodes available tends to 
slow down your computer considerably. Since we'd possibly like to perform some 
other actions while R is occupied (e.g. browsing the internet, checking e-mails, 
etc.), we should leave some remaining computational power for such operations as 
well.  

Now that we're set up properly, it's time to give our cluster something to work 
with. Remember the previous example on calculating the linear relationship 
between 'carat' and 'price' for each 'cut' quality iteratively? We're gonna do the same thing again now 
except this time it's `foreach` we'll be working with. Similar to `*apply`, 
`foreach` requires a set of input data and a function to perform on the very same. 
Have a look at the following example. 

```{r foreach}
## calculate square root, return list
foreach(i = 1:4) %do% sqrt(i)
```

Similar to `lapply`, `foreach` returns an object of class 'list' by default. The 
function body, by contrast, is connected with the part on variable 
definition via the binary operator `%do%`. This might seem odd at first, but you 
will notice later on that this syntax comes in quite handy when forwarding 
`foreach`-related operations to a parallel cluster. 

We may easily change the output type of `foreach` by telling R how to `.combine` 
the results of the single iterations via 

```{r combine}
## calculate sqare root, return vector
foreach(i = 1:4, .combine = "c") %do% sqrt(i)
```

et voilÃ , we transformed the `lapply`-style list output of our loop into an 
`sapply`-style vector output.  

Since you may just take the function body from the previous application with 
`lapply`, it's now up to you to calculate the linear model between 'carat' and 
'price' for each group of 'cut'. Remember to `split` the 'diamonds' dataset 
first and pass the thus created list (with each list entry representing a group 
of uniform cuts) to `foreach`.

<center>
  <img src="https://pixabay.com/static/uploads/photo/2012/04/14/14/04/hourglass-34048_640.png" alt="hourglass" style="width: 200px;"/>
</center>

```{r lm_foreach, eval = FALSE, echo = FALSE}
## load 'ggplot2' and split 'diamonds' dataset
library(ggplot2)
ls_diamonds <- split(diamonds, f = diamonds$cut)

## for each group of cuts, calculate lm between carat and price
ls_lm <- foreach(i = ls_diamonds) %do% lm(carat ~ price, data = i)

## using `sapply`, extract the corresponding r-squared values
rsq <- sapply(ls_lm, function(i) summary(i)$r.squared)
names(rsq) <- unique(diamonds$cut)
rsq
```

Let's see how long this code chunk takes to perform. The **microbenchmark** 
package is just the right thing for such an operation.

```{r lm_foreach_mb}
## load 'microbenchmark' package
library(microbenchmark)

## load 'ggplot2' and split 'diamonds' dataset
library(ggplot2)
ls_diamonds <- split(diamonds, f = diamonds$cut)

## speed test (this might take some time)
microbenchmark({
  foreach(i = ls_diamonds) %do% lm(carat ~ price, data = i)
}, times = 20L) # number of times to evaluate the expression
```

Hm, quite some time... if we could only speed things up a little b---
Oh wait, WE CAN!

```{r lm_foreach_par_mb}
microbenchmark({
  foreach(i = ls_diamonds) %dopar% lm(carat ~ price, data = i)
}, times = 20L)
```

By replacing the binary operator `%do%` with `%dopar%`, we can tell R to outsource the `lm` task and operate on the initially created cluster `cl`. Since we're  